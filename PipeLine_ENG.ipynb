{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8ab823",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d542a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Mydramalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe70986a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:27:34.094645Z",
     "start_time": "2022-05-18T00:27:33.598821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aef156",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "options =  webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_driver_path = '/opt/homebrew/Caskroom/chromedriver/100.0.4896.60/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e94aef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scroll_down(second):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(second)\n",
    "    new = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50aadaa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "for k,v in urls.items():\n",
    "    reviews_dict = {\n",
    "    'title' : [],\n",
    "    'reviews' : []\n",
    "    }\n",
    "    driver.get(v)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    time.sleep(2)\n",
    "    while True:\n",
    "        new_height = scroll_down(2)\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                button = driver.find_element_by_css_selector('#cmtsapp > div.box-footer > button')\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                break\n",
    "        last_height = new_height\n",
    "    \n",
    "    buttons = driver.find_elements_by_class_name('reveal')\n",
    "    for button in tqdm(buttons):\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    reviews = soup.find_all(\"div\", {\"class\": \"post-message\"})\n",
    "    \n",
    "    for review in reviews:\n",
    "        reviews_dict['title'].append(k)\n",
    "        reviews_dict['reviews'].append(review.p.text)\n",
    "    \n",
    "    df = pd.DataFrame(reviews_dict)\n",
    "    csv = df.to_csv(f\"/Users/suchan/study/파이널 프로젝트/{k}_mydramalist.csv\")\n",
    "    csv = pd.read_csv(f\"/Users/suchan/study/파이널 프로젝트/{k}_mydramalist.csv\")\n",
    "    csv.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"{k} 끝!!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165314ec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Letterboxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d126fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_dict = {'스물다섯 스물하나' : 'https://letterboxd.com/film/twenty-five-twenty-one/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da281d40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reviews_dic = {'title' : [], 'reviews' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf31f8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "for k,v in drama_dict.items():\n",
    "    url = v\n",
    "    page_break = False\n",
    "    for i in range(1,100):\n",
    "        driver.get(url+f'reviews/by/activity/page/{i}/')\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # 타이틀 갖고오기\n",
    "        title = driver.find_element_by_xpath('/html/body/div[3]/div/div/section/header/div/h1/a').text\n",
    "        \n",
    "        # more랑 handle 버튼 누르기 없으면 그대로 진행\n",
    "        try:\n",
    "            buttons = driver.find_elements_by_class_name('js-reveal')\n",
    "            for button in buttons:\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # 리뷰 갖고오기\n",
    "        for j in range(1, 13):\n",
    "            try:\n",
    "                review = driver.find_element_by_xpath(f'/html/body/div[3]/div/div/section/section/ul/li[{j}]/div/div[2]').text\n",
    "                reviews_dic['title'].append(title)\n",
    "                reviews_dic['reviews'].append(review)\n",
    "            except:\n",
    "                # 리뷰값이 더 없으면 리뷰 for문과 페이지 for문 중단\n",
    "                page_break = True\n",
    "                break\n",
    "        if (page_break == True):\n",
    "            break\n",
    "                \n",
    "    time.sleep(2)\n",
    "\n",
    "print(f'{k} 끝!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8a3e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e350a898",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d4c8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/opt/homebrew/Caskroom/chromedriver/99.0.4844.51/chromedriver')\n",
    "driver.set_page_load_timeout(\"20\")\n",
    "\n",
    "# 작품 구분없이 장르별 드라마 10개의 리뷰 가져오기\n",
    "for i in range(3):\n",
    "    for j in range(10):\n",
    "        url = WHOLE_URL[i + 4][j]\n",
    "        time.sleep(5)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 작품 제목\n",
    "#         title = driver.find_element_by_css_selector('#main > section > div.subpage_title_block > div > div > h3 > a').text\n",
    "#         print(\"title\", title)\n",
    "        \n",
    "        # 댓글 수\n",
    "        review_counts = driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/section/div[2]/div[1]/div/span').text\n",
    "        review_counts = int(review_counts.split(' ')[0].replace(',', ''))\n",
    "        \n",
    "        # Load More 끝날 때까지 반복\n",
    "        for k in range(review_counts):    \n",
    "            try:\n",
    "                driver.find_element_by_class_name(\"ipl-load-more__button\").send_keys(Keys.ENTER)\n",
    "                time.sleep(3)\n",
    "            except Exception as ex:\n",
    "                print('더이상 \"Load More\" 없음', ex)\n",
    "                break\n",
    "\n",
    "        # 댓글 수집\n",
    "        print(\" 댓글 수집 \")\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            soup= BeautifulSoup(driver.page_source)\n",
    "            time.sleep(1)\n",
    "\n",
    "            comment=[]\n",
    "            x= soup.find_all(\"div\", class_=\"content\")\n",
    "\n",
    "            for h in x:\n",
    "                try:\n",
    "                    tmp= h.find_all(\"div\", class_=\"text show-more__control\")[0].text\n",
    "                    review_dic['genre'].append(genre[i + 4])\n",
    "                    review_dic['review'].append(tmp)\n",
    "                    \n",
    "                except Exception as ex:\n",
    "                    tmp= h.find_all(\"div\", class_=\"text show-more__control clickable\")[0].text\n",
    "                    review_dic['genre'].append(genre[i + 4])\n",
    "                    review_dic['review'].append(tmp)\n",
    "\n",
    "            time.sleep(1)\n",
    "        except Exception as ex:\n",
    "            re=['nothing']\n",
    "            print(\"댓글없음\")\n",
    "\n",
    "        # 한줄평\n",
    "        print(\"한줄평 수집 \")\n",
    "        try:\n",
    "            oneline=[]\n",
    "            x= soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "            for h in x:\n",
    "                ol= h.find_all(\"a\", class_=\"title\")[0].text\n",
    "                review_dic['genre'].append(genre[i + 4])\n",
    "                review_dic['review'].append(ol)\n",
    "                \n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as ex:\n",
    "            oneline= 'nothing'\n",
    "            print(\"\")\n",
    "\n",
    "        time.sleep(1)\n",
    "        print(datetime.now())\n",
    "        print(\"---수집 댓글, 한줄평 저장---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2a538",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229641f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titles = Mydramalist.title.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256c267",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    mydramalist = pd.DataFrame(Mydramalist[Mydramalist.title == title].review.tolist())\n",
    "    letterboxd = pd.DataFrame(Letterboxd[Letterboxd.title == title].review.tolist())\n",
    "    imdb = pd.DataFrame(IMDB[IMDB.title == title].review.tolist())\n",
    "    df = pd.concat([mydramalist,letterboxd,imdb], axis=0)\n",
    "    df.to_csv(f\"{title}_reviews.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6bcbe7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abe835",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i in df_watcha['reviews']:\n",
    "    text = re.sub('[^a-zA-Z\\']',' ',i).strip()\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》]','', text)\n",
    "    # text = re.sub(' +', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd58cbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng_frame = pd.DataFrame(sentences, columns = ['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd75b0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples = pd.Series(eng_frame['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db471",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_length = samples.apply(len)\n",
    "eng_frame['lens'] = df_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b15710",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('리뷰 길이 최대값 : {}'.format(np.max(eng_frame['lens'])))\n",
    "print('리뷰 길이 최소값 : {}'.format(np.min(eng_frame['lens'])))\n",
    "print('리뷰 길이 평균값 : {:.2f}'.format(np.mean(eng_frame['lens'])))\n",
    "print('리뷰 길이 표준편차 : {:.2f}'.format(np.std(eng_frame['lens'])))\n",
    "print('리뷰 길이 중간값 : {}'.format(np.median(eng_frame['lens'])))\n",
    "print('리뷰 길이 제1사분위 : {}'.format(np.percentile(eng_frame['lens'], 25)))\n",
    "print('리뷰 길이 제3사분위 : {}'.format(np.percentile(eng_frame['lens'], 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d7aa3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "length = np.percentile(df_length, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a3ef4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng_reviews= []\n",
    "for index, i in enumerate(df_length):\n",
    "    if i > length and i < 13000:\n",
    "        eng_reviews.append(eng_frame.reviews[index])\n",
    "    else:\n",
    "        eng_reviews.append('빈값')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91ad27",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_eng = pd.DataFrame(eng_reviews, columns = ['reviews'])\n",
    "df_eng['title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72958d5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_eng.drop(df_eng[df_eng['reviews'] == '빈값'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d10694",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_eng['title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18c17d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33d9ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5955b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ad071",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Lang'] = df['reviews'].apply(det)\n",
    "en_reviews = (df['Lang'] == 'en')\n",
    "df_en_reviews = df[en_reviews]['reviews']\n",
    "detect_list = df_en_reviews.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0c197",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165abe0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, a):\n",
    "        a = ''.join(a)\n",
    "        word_tokens = self.tagger(a)\n",
    "        \n",
    "        words = []\n",
    "\n",
    "        for i in word_tokens:\n",
    "            text = re.sub('[^a-zA-Z0-9\\']','',i).strip()\n",
    "            text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》]','', text)\n",
    "            if(text != ''):\n",
    "                words.append(text)\n",
    "        \n",
    "        tag_words = nltk.pos_tag(words)\n",
    "        # pos_words = [word for word in tag_words if word[1][0] in {'V','N','J','R'}]\n",
    "        pos_words = []\n",
    "        for word in tag_words:\n",
    "            if word[1] in ['NNP', 'NN']:  #'VB', 'VBP', 'JJ'\n",
    "                pos_words.append(word)\n",
    "        temp_list = []\n",
    "        for token, pos_tag in pos_words:\n",
    "            tag = get_wordnet_pos(pos_tag)\n",
    "            if tag != None:\n",
    "                temp_list.append((token, get_wordnet_pos(pos_tag)))\n",
    "        lemma = WordNetLemmatizer()\n",
    "        token_final = [lemma.lemmatize(token, pos=tag) for token, tag in temp_list]\n",
    "        long_words = [i for i in token_final if len(i) > 2]\n",
    "        results = [w for w in long_words if w not in stop_words]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda8c63",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "custom_tokenizer = CustomTokenizer(text_to_word_sequence)\n",
    "for sentences in detect_list:\n",
    "    tokens.append(custom_tokenizer(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfacb2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Korean Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698440d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Review Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18603b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_lower = df['reviews'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46f58b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_word_list = set(['korean idol', ' korean star', 'running man', 'nct', 'blackpink', 'bts', 'exo', 'exo l',\n",
    "                      'iu', 'bona', 'wjsn', 'kdrama look', 'hanbok', 'korean clothing', 'sseugaechima', 'outfits', 'airport outfit',\n",
    "                      'chimaek', 'chimac', 'mukbang', 'kimchi', 'kimchee', 'kimchijigae', 'ramen', 'hansik', 'haejangguk',\n",
    "                      'korean cook', 'korean bbq', 'korean barbeque', 'soju', 'korean drinks', 'korean noodle', 'ksool', 'sool',\n",
    "                      'dalgona', 'makgeolli', 'cold noodles', 'kfood', 'tteokbokki', 'topokki', 'ramyeon', 'samgyeopsal',\n",
    "                      'bossam', 'ost', 'by taeyeon', 'kpop', 'kbeauty', 'korean beauty', 'oppa', 'unni', 'unnie', 'noona',\n",
    "                      'daebak', 'heol', 'jjang', 'aegyo', 'simkung', 'kkuljaem', 'nojaem', 'hwaiting', 'menbung', 'selka',\n",
    "                      'jjeoreo', 'eotteoke', 'sesangae', 'nunting', 'bulgeum', 'matjeom', 'honsool', 'honsul', 'honbab', 'jonmat', 'kol',\n",
    "                      'ssum', 'sseom', 'mossol', 'tmi', 'green light', 'gapjil', 'kkondae', 'deokhu', 'deokjil', 'finger heart', 'maknae',\n",
    "                      'mujigae', 'chaebol', 'gajima', 'makjang', 'gganbu', 'fighting', 'vincenzo', 'moon embracing the sun', 'hallyu', 'korean wave',\n",
    "                      'hide and seek', 'chingu', 'manhwa', 'slowburn', 'slow burn', 'kzombie', 'south korea', 's korea', 'south koreas', 'parasite', 'gwangju uprising',\n",
    "                      'north korea', 'sageuk', 'sageuks', 'marbles', 'seoul', 'busan', 'kdrama', 'kbs', 'sbs', 'mbc', 'jtbc', 'tvn', 'korean', 'koreans',\n",
    "                      'chosun', 'joseon', 'manga', 'ganbu', 'incheon', 'daejeon', 'daegu', 'gangnam', 'jeonju', 'jeollabuk do', 'gyeonggi', 'dongseong',\n",
    "                      'korea', 'hanguk', 'baeksang', 'ddakji', 'joseon', 'goryeo', 'gojong', ' shinmiyangyo', 'kdramas', 'cosmetic surgery', 'plastic surgery',\n",
    "                      'jjampong', 'jajangmyeon', 'nurungji', 'gukbap', 'kimbap', 'bibimbap', 'sikhye', 'gimbap', 'lee sun hee', 'osts', 'lee sun hees', 'taeyeon',\n",
    "                      'ben', 'davichi', 'punch', 'sheet mask', 'zombie makeup', 'lipstick', 'netizen', 'ahjumma', 'ajeossi', 'skinship', 'morning call',\n",
    "                      'gimpo', 'gimhae', 'guro', 'gumi', 'gwacheon', 'sungkyunkwan', 'jeju', 'hwaseong', 'sungkyunkwan scandal', 'yi san', 'yi sans', 'sado',\n",
    "                      'hwaseong killer', 'hwaseong serial killer', 'taekwondo', 'tug of war', 'ktweet', 'kwon il yong', 'taeyeons', 'tteok', 'kang ho sun',\n",
    "                      'kwon ilyongs', 'itzy', 'enhypen', 'nuest', 'girls generation', 'clothing', 'outfit', 'fashion show', 'smart fashion', 'banchan',\n",
    "                      'ailee', 'gaja', 'train to busan', 'dream high', 'producers', 'hanyang', 'jeon bong jun', 'donghak', 'raincoat killer', 'beanpole',\n",
    "                      'uniform', 'lee san', 'track record', 'bigbang', 'minari', 'taegeuk', 'imjin', 'chuseok', 'nongshim', 'itaewon', 'hongdae',\n",
    "                     'appa', 'umma', 'eomma', 'dongsaeng', 'dongseng', 'imo', 'sajang', 'saranghe', 'sunbae', 'seonbae', 'samchon', 'halmeoni', 'halabeoji',\n",
    "                      'jinttobaegi', 'sseraegi', 'podaegi', 'sangsa', 'lee moon saes', 'lee moon sae', 'seunghwan', 'melomance', 'sondia', 'heize',\n",
    "                      'hwasa', 'do bong soon', 'kim eun hee', 'goguryeo', 'silla', 'baekje', 'baekhyun', 'mamamoo', 'kim eunsook', 'kim eun sook',\n",
    "                      'kim eun suk', 'kim eunsuk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68743312",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "K_dict = {'K_words_list' : []}\n",
    "for review in reviews:\n",
    "    word_list = []\n",
    "    for word in find_word_list:\n",
    "        if re.search(f\"[\\W]{word}[\\W]\",review):\n",
    "            word_list.append(word)\n",
    "    if len(word_list) != 0:\n",
    "        K_dict['K_words_list'].append(word_list)\n",
    "    else:\n",
    "        K_dict['K_words_list'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374279b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_K_dict = pd.DataFrame(K_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ea3be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ = df_K_dict.replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e7a1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_korean_flavor = pd.concat([df_lower,df_], axis=1)\n",
    "df_korean_flavor.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2070f32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_food = [\n",
    "    'chimaek', 'chimac', 'mukbang', 'kimchi', 'kimchee', 'kimchijigae', 'ramen', 'hansik', 'haejangguk', 'korean cook', 'korean bbq',\n",
    "    'korean barbeque', 'soju', 'korean drinks', 'korean noodle', 'ksool', 'sool', 'makgeolli', 'cold noodles', 'kfood', 'tteokbokki',\n",
    "    'topokki', 'ramyeon', 'samgyeopsal', 'jjampong', 'jajangmyeon', 'gimbap', 'nurungji', 'gukbap', 'kimbap', 'bibimbap', 'sikhye',\n",
    "    'tteok', 'banchan', 'nongshim', 'bossam'\n",
    "]\n",
    "\n",
    "place = [\n",
    "    'south korea', 's korea', 'south koreas', 'north korea', 'seoul', 'busan', 'incheon', 'daejeon', 'daegu', 'gangnam',\n",
    "    'jeonju', 'jeollabuk do', 'gyeonggi', 'dongseong', 'korea', 'hanguk', 'gimpo', 'gimhae', 'guro', 'gumi', 'gwacheon',\n",
    "    'sungkyunkwan', 'jeju', 'hwaseong', 'hanyang', 'itaewon', 'hongdae'\n",
    "]\n",
    "\n",
    "k_slang = [\n",
    "    'oppa', 'unni', 'unnie', 'noona', 'daebak', 'heol', 'jjang', 'aegyo', 'simkung', 'kkuljaem', 'nojaem', 'hwaiting', 'menbung', 'selka',\n",
    "    'jjeoreo', 'eotteoke', 'sesangae', 'nunting', 'bulgeum', 'matjeom', 'honsool', 'honsul', 'honbab', 'jonmat', 'kol', 'ssum', 'sseom',\n",
    "    'mossol', 'green light', 'gapjil', 'kkondae', 'deokhu', 'deokjil', 'maknae', 'mujigae', 'chaebol', 'gajima', 'makjang', 'gganbu', 'ganbu',\n",
    "    'netizen', 'ahjumma', 'ajeossi', 'skinship', 'morning call', 'gaja', 'appa', 'umma', 'eomma', 'dongsaeng', 'dongseng', 'imo', 'sajang', 'saranghe',\n",
    "    'sunbae', 'seonbae', 'samchon', 'halmeoni', 'halabeoji', 'jinttobaegi', 'sseraegi', 'podaegi', 'sangsa', 'fighting'\n",
    "]\n",
    "\n",
    "k_fashion_beauty = [\n",
    "    'kdrama look', 'hanbok', 'korean clothing', 'sseugaechima', 'beanpole', 'kbeauty', 'korean beauty', 'sheet mask', 'zombie makeup',\n",
    "    'outfits', 'airport outfit', 'clothing', 'outfit', 'fashion show', 'smart fashion', 'uniform', 'cosmetic surgery', 'plastic surgery', 'lipstick'\n",
    "]\n",
    "\n",
    "k_pop = [\n",
    "    'ost', 'lee sun hee', 'osts', 'lee sun hees', 'taeyeon', 'taeyeons', 'ben', 'davichi', 'ailee', 'nuest', 'track record',\n",
    "    'lee moon saes', 'lee moon sae', 'seunghwan', 'melomance', 'sondia', 'heize', 'baekhyun', 'korean idol', 'korean star', 'nct',\n",
    "    'blackpink', 'bts', 'd.o.', 'exo', 'exo l', 'iu', 'bona', 'wjsn', 'tvxq', 'itzy', 'enhypen', 'girls generation', 'bigbang', 'hwasa', 'mamamoo'\n",
    "]\n",
    "\n",
    "history_event = [\n",
    "    'joseon', 'goryeo', 'gojong', 'shinmiyangyo', 'gwangju uprising',\n",
    "    'yi san', 'yi sans', 'sado', 'kwon ilyongs', 'kwon il yong', 'kang ho sun', 'hwaseong killer',\n",
    "    'hwaseong serial killer', 'jeon bong jun', 'donghak', 'raincoat killer', 'lee san', 'imjin', 'chosun', 'goguryeo', 'silla', 'baekje'\n",
    "]\n",
    "\n",
    "k_culture = [\n",
    "    'vincenzo', 'moon embracing the sun', 'parasite', 'train to busan', 'dream high', 'producers', 'minari', 'running man',\n",
    "    'sungkyunkwan scandal', 'do bong soon', 'hallyu', 'korean wave', 'hide and seek', 'chingu', 'manhwa', 'slowburn', 'slow burn',\n",
    "    'kzombie', 'sageuk', 'sageuks', 'marbles', 'baeksang', 'ddakji', 'taekwondo', 'finger heart', 'dalgona', 'taegeuk', 'chuseok',\n",
    "    'kim eun hee', 'kim eunsook', 'kim eun sook', 'kim eun suk', 'kim eunsuk', 'kdrama', 'kdramas', 'kbs', 'sbs', 'mbc', 'jtbc', 'tvn',\n",
    "    'korean', 'koreans', 'manga', 'tug of war', 'k tweet'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2ea57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "categories = [k_food, place, k_slang, k_fashion_beauty, k_pop, history_event,k_culture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14201d57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numpy1 = np.zeros((27699,7))\n",
    "category_df = pd.DataFrame(numpy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de60ae6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_korean_flavor.reset_index(inplace=True)\n",
    "df_korean_flavor.drop(['index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc994b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    for i in tqdm(range(len(df_korean_flavor))):\n",
    "        length = len(df_korean_flavor.K_words_list[i])\n",
    "        for j in range(length):\n",
    "            if df_korean_flavor.K_words_list[i][j] in category:\n",
    "                category_df.iloc[i][categories.index(category)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c8442",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "category_df.columns= ['k_food', 'place', 'k_slang', 'k_fashion_beauty', 'k_pop', 'history_event', 'k_culture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76a0bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_korean = pd.concat([df_korean_flavor,category_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a74ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_korean.to_csv(\"ENG_Korean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd214bc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Count Topic & Rank Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a94671",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "korea_senti = pd.read_csv('/content/drive/MyDrive/0506_ENG_Korean_plus_Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff34e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_food = []\n",
    "place = []\n",
    "k_slang = []\n",
    "k_fashion_beauty = []\n",
    "k_pop = []\n",
    "history_event = []\n",
    "k_culture = []\n",
    "titles = []\n",
    "for title in korea_senti['title'].unique():\n",
    "    senti = korea_senti[korea_senti['title'] == title]\n",
    "    k_food.append(senti['k_food'].sum())\n",
    "    place.append(senti['place'].sum())\n",
    "    k_slang.append(senti['k_slang'].sum())\n",
    "    k_fashion_beauty.append(senti['k_fashion_beauty'].sum())\n",
    "    k_pop.append(senti['k_pop'].sum())\n",
    "    history_event.append(senti['history_event'].sum())\n",
    "    k_culture.append(senti['k_culture'].sum())\n",
    "    titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e7221",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(titles, k_food, place, k_slang, k_fashion_beauty, k_pop, history_event, k_culture, df.sum(axis=1))), columns = ['titles', 'k_food', 'place', 'k_slang', 'k_fashion_beauty', 'k_pop', 'history_event', 'k_culture', 'keywords_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3013c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ratio_food = df['k_food']/df['keywords_sum']\n",
    "ratio_place = df['place']/df['keywords_sum']\n",
    "ratio_slang = df['k_slang']/df['keywords_sum']\n",
    "ratio_fashion_beauty = df['k_fashion_beauty']/df['keywords_sum']\n",
    "ratio_pop = df['k_pop']/df['keywords_sum']\n",
    "ratio_history_event = df['history_event']/df['keywords_sum']\n",
    "ratio_culture = df['k_culture']/df['keywords_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae74182",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['ratio_food'] = ratio_food\n",
    "df['ratio_place'] = ratio_place\n",
    "df['ratio_slang'] = ratio_slang\n",
    "df['ratio_fashion_beauty'] = ratio_fashion_beauty\n",
    "df['ratio_pop'] = ratio_pop\n",
    "df['ratio_history_event'] = ratio_history_event\n",
    "df['ratio_culture'] = ratio_culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f20ed2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rank_food = df['ratio_food'].rank(method='min', ascending=False)\n",
    "rank_place = df['ratio_place'].rank(method='min', ascending=False)\n",
    "rank_slang = df['ratio_slang'].rank(method='min', ascending=False)\n",
    "rank_fashion_beauty = df['ratio_fashion_beauty'].rank(method='min', ascending=False)\n",
    "rank_pop = df['ratio_pop'].rank(method='min', ascending=False)\n",
    "rank_history_event = df['ratio_history_event'].rank(method='min', ascending=False)\n",
    "rank_culture = df['ratio_culture'].rank(method='min', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579e720",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['rank_food'] = rank_food\n",
    "df['rank_place'] = rank_place\n",
    "df['rank_slang'] = rank_slang\n",
    "df['rank_fashion_beauty'] = rank_fashion_beauty\n",
    "df['rank_pop'] = rank_pop\n",
    "df['rank_history_event'] = rank_history_event\n",
    "df['rank_culture'] = rank_culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c9b7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng_group = pd.read_csv('/content/drive/MyDrive/ENG_drama_group.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f544db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_rank = eng_group[['titles', 'rank_food', 'rank_place', 'rank_slang', 'rank_fashion_beauty', 'rank_pop', 'rank_history_event', 'rank_culture']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d769306",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df.replace({'rank_food' : 43}, 100, inplace=True)\n",
    "copy_df.replace({'rank_place' : 92}, 100, inplace=True)\n",
    "copy_df.replace({'rank_slang' : 92}, 100, inplace=True)\n",
    "copy_df.replace({'rank_fashion_beauty' : 68}, 100, inplace=True)\n",
    "copy_df.replace({'rank_pop' : 91}, 100, inplace=True)\n",
    "copy_df.replace({'rank_history_event' : 40}, 100, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00919de7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rank_copy = df_rank.copy()\n",
    "rank_copyt = rank_copy.T\n",
    "rank_list = rank_copy['titles'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176cfbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rank_copyt.columns = rank_list\n",
    "rank_copyt = rank_copyt.drop('titles', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202b147",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Drama Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e11c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "under_20 = []\n",
    "ind = rank_copyt['index']\n",
    "for title in rank_list:\n",
    "    temp = []\n",
    "    index_list = np.argsort(rank_copyt[title])\n",
    "    for index in tqdm(index_list[:2]):\n",
    "        if (rank_copyt[title].iloc[index] <= 20) & (len(temp) <= 1):\n",
    "            temp.append(ind[index])\n",
    "            # rank_sort = np.argsort(rank_copyt[title])\n",
    "            # temp.append(rank_copyt['index'][rank_sort][:2])\n",
    "    if temp == []:\n",
    "        under_20.append((title, [ind[np.argmin(rank_copyt[title])]]))\n",
    "    else:\n",
    "        under_20.append((title, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc7eb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_food = []\n",
    "place = []\n",
    "k_slang = []\n",
    "k_fashion_beauty = []\n",
    "k_pop = []\n",
    "history_event = []\n",
    "k_culture = []\n",
    "for drama, category in under_20:\n",
    "    for i in category:\n",
    "        if i == 'rank_food':\n",
    "            k_food.append(drama)\n",
    "        elif i == 'rank_place':\n",
    "            place.append(drama)\n",
    "        elif i == 'rank_slang':\n",
    "            k_slang.append(drama)\n",
    "        elif i == 'rank_fashion_beauty':\n",
    "            k_fashion_beauty.append(drama)\n",
    "        elif i == 'rank_pop':\n",
    "            k_pop.append(drama)\n",
    "        elif i == 'rank_history_event':\n",
    "            history_event.append(drama)\n",
    "        else:\n",
    "            k_culture.append(drama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0e93f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_dict = {'k_food': [], 'place': [], 'k_slang': [], 'k_fashion_beauty': [\n",
    "], 'k_pop': [], 'history_event': [], 'k_culture': []}\n",
    "for drama, category in under_20:\n",
    "    for i in category:\n",
    "        if i == 'rank_food':\n",
    "            k_dict['k_food'].append(drama)\n",
    "        elif i == 'rank_place':\n",
    "            k_dict['place'].append(drama)\n",
    "        elif i == 'rank_slang':\n",
    "            k_dict['k_slang'].append(drama)\n",
    "        elif i == 'rank_fashion_beauty':\n",
    "            k_dict['k_fashion_beauty'].append(drama)\n",
    "        elif i == 'rank_pop':\n",
    "            k_dict['k_pop'].append(drama)\n",
    "        elif i == 'rank_history_event':\n",
    "            k_dict['history_event'].append(drama)\n",
    "        else:\n",
    "            k_dict['k_culture'].append(drama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0e12c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83855c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb579273",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977810e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value+3] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cf832",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index] = token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11adeef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93322e8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "vocab_size = 10000\n",
    "max_len = 500\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40203eff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2502b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Embedding(10000,100,input_length=max_len),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65fe49",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('BiLSTM_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff8894",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "     loss='binary_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b22718",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978f565",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loaded_model = load_model('/Users/suchan/study/Final_Project/0506_GRU_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843eb3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    # 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\n",
    "    new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\n",
    "    encoded = []\n",
    "\n",
    "    # 띄어쓰기 단위 토큰화 후 정수 인코딩\n",
    "    for word in new_sentence.split():\n",
    "        try:\n",
    "            # 단어 집합의 크기를 10,000으로 제한.\n",
    "            if word_to_index[word] <= 10000:\n",
    "                encoded.append(word_to_index[word]+3)\n",
    "            else:\n",
    "                # 10,000 이상의 숫자는 <unk> 토큰으로 변환.\n",
    "                encoded.append(2)\n",
    "        # 단어 집합에 없는 단어는 <unk> 토큰으로 변환.\n",
    "        except:\n",
    "            encoded.append(2)\n",
    "\n",
    "    pad_sequence = pad_sequences([encoded], maxlen=max_len)\n",
    "    score = float(loaded_model.predict(pad_sequence))  # 예측\n",
    "\n",
    "    if(score > 0.6):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d8ee9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos_neg = []\n",
    "for i in tqdm(range(len(eng['reviews']))):\n",
    "    sentiment = sentiment_predict(eng['reviews'][i])\n",
    "    pos_neg.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26afa49",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng['pos_neg'] = pos_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28de99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng.to_csv('0506_ENG_Korean_plus_Sentiment.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f86391",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9613f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/0506_ENG_Korean_plus_Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa3e6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_theme = [('악의마음을읽는자들', ['rank_history_event', 'rank_place']),\n",
    "    ('라이프온마스', ['rank_place', 'rank_food']),\n",
    "    ('옷소매붉은끝동', ['rank_history_event']),\n",
    "    ('이구역의미친X', ['rank_food', 'rank_fashion_beauty']),\n",
    "    ('어느날우리집현관으로멸망이들어왔다', ['rank_pop', 'rank_food']),\n",
    "    ('스물다섯스물하나', ['rank_pop']),\n",
    "    ('무브투헤븐:나는유품정리사입니다', ['rank_culture']),\n",
    "    ('호텔델루나', ['rank_pop', 'rank_fashion_beauty']),\n",
    "    ('하늘에서내리는일억개의별', ['rank_culture']),\n",
    "    ('기름진멜로', ['rank_food', 'rank_slang']),\n",
    "    ('악의꽃', ['rank_culture']),\n",
    "    ('서른이지만열일곱입니다', ['rank_pop']),\n",
    "    ('킹덤', ['rank_history_event', 'rank_place']),\n",
    "    ('작은신의아이들', ['rank_culture']),\n",
    "    ('검색어를입력하세요WWW', ['rank_slang', 'rank_pop']),\n",
    "    ('술꾼도시여자들', ['rank_food', 'rank_place']),\n",
    "    ('한번다녀왔습니다', ['rank_food', 'rank_slang']),\n",
    "    ('서른아홉', ['rank_food', 'rank_slang']),\n",
    "    ('오월의청춘', ['rank_history_event', 'rank_place']),\n",
    "    ('트랩', ['rank_slang']),\n",
    "    ('왓쳐', ['rank_food', 'rank_pop']),\n",
    "    ('아는와이프', ['rank_slang', 'rank_pop']),\n",
    "    ('자백', ['rank_culture']),\n",
    "    ('60일지정생존자', ['rank_place']),\n",
    "    ('시맨틱에러', ['rank_culture']),\n",
    "    ('녹두꽃', ['rank_history_event']),\n",
    "    ('백일의낭군님', ['rank_history_event', 'rank_pop']),\n",
    "    ('괴물', ['rank_food', 'rank_history_event']),\n",
    "    ('태종이방원', ['rank_culture', 'rank_history_event']),\n",
    "    ('사이코지만괜찮아', ['rank_fashion_beauty', 'rank_food']),\n",
    "    ('런온', ['rank_food']),\n",
    "    ('365:운명을거스르는1년', ['rank_slang']),\n",
    "    ('시를잊은그대에게', ['rank_culture']),\n",
    "    ('인간수업', ['rank_place']),\n",
    "    ('으라차차와이키키', ['rank_culture']),\n",
    "    ('우수무당가두심', ['rank_place', 'rank_history_event']),\n",
    "    ('트레이서', ['rank_culture']),\n",
    "    ('사내맞선', ['rank_culture', 'rank_food']),\n",
    "    ('구경이', ['rank_pop', 'rank_food']),\n",
    "    ('어쩌다발견한하루', ['rank_history_event', 'rank_culture']),\n",
    "    ('김비서가왜그럴까', ['rank_fashion_beauty', 'rank_food']),\n",
    "    ('나의아저씨', ['rank_pop']),\n",
    "    ('카이로스', ['rank_culture', 'rank_slang']),\n",
    "    ('18어게인', ['rank_pop']),\n",
    "    ('D.P.', ['rank_place']),\n",
    "    ('로스쿨', ['rank_pop']),\n",
    "    ('대박부동산', ['rank_slang', 'rank_fashion_beauty']),\n",
    "    ('사랑의불시착', ['rank_place']),\n",
    "    ('스토브리그', ['rank_place', 'rank_fashion_beauty']),\n",
    "    ('단하나의사랑', ['rank_pop', 'rank_slang']),\n",
    "    ('안녕나야', ['rank_slang', 'rank_fashion_beauty']),\n",
    "    ('그림자미녀', ['rank_fashion_beauty', 'rank_culture']),\n",
    "    ('도시남녀의사랑법', ['rank_place']),\n",
    "    ('본대로말하라', ['rank_culture']),\n",
    "    ('내뒤에테리우스', ['rank_slang']),\n",
    "    ('월간집', ['rank_food']),\n",
    "    ('비밀의숲', ['rank_food', 'rank_place']),\n",
    "    ('나빌레라', ['rank_fashion_beauty']),\n",
    "    ('슬기로운의사생활', ['rank_food', 'rank_pop']),\n",
    "    ('스카이캐슬', ['rank_place']),\n",
    "    ('하이에나', ['rank_food', 'rank_fashion_beauty']),\n",
    "    ('라이브', ['rank_place']),\n",
    "    ('갯마을차차차', ['rank_food']),\n",
    "    ('소년심판', ['rank_place']),\n",
    "    ('그녀의사생활', ['rank_fashion_beauty']),\n",
    "    ('손theguest', ['rank_food']),\n",
    "    ('세빛남고학생회', ['rank_fashion_beauty', 'rank_place']),\n",
    "    ('오징어게임', ['rank_culture']),\n",
    "    ('원더우먼', ['rank_slang', 'rank_fashion_beauty']),\n",
    "    ('우아한친구들', ['rank_pop']),\n",
    "    ('낭만닥터김사부', ['rank_pop']),\n",
    "    ('마우스', ['rank_food', 'rank_fashion_beauty']),\n",
    "    ('지옥', ['rank_place']),\n",
    "    ('왕이된남자', ['rank_history_event', 'rank_culture']),\n",
    "    ('라켓소년단', ['rank_culture']),\n",
    "    ('유미의세포들', ['rank_fashion_beauty']),\n",
    "    ('부부의세계', ['rank_slang']),\n",
    "    ('로맨스는별책부록', ['rank_slang', 'rank_fashion_beauty']),\n",
    "    ('미스티', ['rank_fashion_beauty']),\n",
    "    ('미치지않고서야', ['rank_culture']),\n",
    "    ('마녀식당으로오세요', ['rank_place', 'rank_fashion_beauty']),\n",
    "    ('미스터션샤인', ['rank_history_event']),\n",
    "    ('모범택시', ['rank_history_event']),\n",
    "    ('사의찬미', ['rank_fashion_beauty', 'rank_pop']),\n",
    "    ('멜로가체질', ['rank_pop']),\n",
    "    ('이리와안아줘', ['rank_fashion_beauty', 'rank_slang']),\n",
    "    ('보좌관:세상을움직이는사람들', ['rank_pop']),\n",
    "    ('뷰티인사이드', ['rank_pop', 'rank_slang']),\n",
    "    ('우리들의블루스', ['rank_place', 'rank_history_event']),\n",
    "    ('해피니스', ['rank_history_event']),\n",
    "    ('의사요한', ['rank_pop']),\n",
    "    ('동백꽃필무렵', ['rank_culture'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41537358",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples = df.copy()\n",
    "samples.columns\n",
    "samples.rename(columns={'k_food':'rank_food', 'place':'rank_place', 'k_slang' : 'rank_slang',\n",
    "       'k_fashion_beauty':'rank_fashion_beauty', 'k_pop' : 'rank_pop',\n",
    "       'history_event' : 'rank_history_event', 'k_culture' : 'rank_culture'},inplace=True)\n",
    "samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441a657",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos_df = samples[samples.pos_neg == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b9419",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "drama_food_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    food_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_food:\n",
    "            food_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in food_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_food_keywords.append((drama, sort_words))\n",
    "drama_food_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89636f54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_place_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    place_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in place:\n",
    "            place_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in place_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_place_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a018a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_slang_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    slang_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_slang:\n",
    "            slang_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in slang_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_slang_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130400f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_fashion_beauty_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    fashion_beauty_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_fashion_beauty:\n",
    "            fashion_beauty_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in fashion_beauty_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_fashion_beauty_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e86bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_pop_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    pop_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_pop:\n",
    "            pop_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in pop_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_pop_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63c6ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_history_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    history_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_food:\n",
    "            history_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in history_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_history_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9013a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drama_culture_keywords = []\n",
    "for drama in pos_df['title'].unique():\n",
    "    keyword_list = []\n",
    "    for li in pos_df[(pos_df.title == drama)]['K_words_list']:\n",
    "        str_list = ast.literal_eval(li)\n",
    "        for keyword in str_list:\n",
    "            keyword_list.append(keyword)\n",
    "    culture_keywords = []\n",
    "    for i in keyword_list:\n",
    "        if i in k_food:\n",
    "            culture_keywords.append(i)\n",
    "\n",
    "    wordCount = {}\n",
    "\n",
    "    for word in culture_keywords:\n",
    "\n",
    "        # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "\n",
    "        keys = sorted(wordCount.keys())\n",
    "\n",
    "    sort_words = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    drama_culture_keywords.append((drama, sort_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312234a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Review Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098bfd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f32d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad56191",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62331fd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bart_summarize(text, num_beams, length_penalty, max_length, min_length, no_repeat_ngram_size):\n",
    "\n",
    "    text = text.replace('\\n', '')\n",
    "    text_input_ids = tokenizer.batch_encode_plus(\n",
    "        [text], return_tensors='pt', max_length=1024, truncation=True)['input_ids'].to(torch_device)\n",
    "    summary_ids = model.generate(text_input_ids, num_beams=int(num_beams), length_penalty=float(\n",
    "        length_penalty), max_length=int(max_length), min_length=int(min_length), no_repeat_ngram_size=int(no_repeat_ngram_size))\n",
    "    summary_txt = tokenizer.decode(\n",
    "        summary_ids.squeeze(), skip_special_tokens=True)\n",
    "    return summary_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9258d61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162e70f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/suchan/study/Final_Project/0506_ENG_Korean_plus_Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79032c8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(df[df.pos_neg == 1].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a0eb8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reviews = df.reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d1461",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "review_summarize = []\n",
    "for review in tqdm(reviews):\n",
    "    review_summarize.append(bart_summarize(review, 4, 4.0, 142, 5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aaa32c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_review_summarize = pd.DataFrame(review_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31a5da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_review_summarize.to_csv(\"review_summarize.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
